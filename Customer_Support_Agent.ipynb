{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install langchain langchain_core langchain_groq langchain_community langgraph gradio"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Imports and setup\n",
    "from typing import TypedDict, Dict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from IPython.display import display, Image"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define shared state\n",
    "class State(TypedDict):\n",
    "    query: str\n",
    "    category: str\n",
    "    sentiment: str\n",
    "    response: str"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    groq_api_key=\"YOUR_GROQ_API_KEY\",\n",
    "    model_name=\"llama-3.3-70b-versatile\"\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "llm.invoke(\"What is LangChain?\").content"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Node: Categorize query\n",
    "def categorize(state: State) -> State:\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Categorize the following customer query into one of these categories: \"\n",
    "        \"Technical, Billing, General. Query: {query}\"\n",
    "    )\n",
    "    category = (prompt | llm).invoke({\"query\": state['query']}).content\n",
    "    return {\"category\": category}\n",
    "\n",
    "# Node: Analyze sentiment\n",
    "def analyze_sentiment(state: State) -> State:\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Analyze the sentiment of the following customer query. \"\n",
    "        \"Respond with Positive, Neutral, or Negative. Query: {query}\"\n",
    "    )\n",
    "    sentiment = (prompt | llm).invoke({\"query\": state['query']}).content\n",
    "    return {\"sentiment\": sentiment}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Support handlers\n",
    "def handle_technical(state: State) -> State:\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Provide a technical support response to the following query: {query}\"\n",
    "    )\n",
    "    response = (prompt | llm).invoke({\"query\": state['query']}).content\n",
    "    return {\"response\": response}\n",
    "\n",
    "def handle_billing(state: State) -> State:\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Provide a billing support response to the following query: {query}\"\n",
    "    )\n",
    "    response = (prompt | llm).invoke({\"query\": state['query']}).content\n",
    "    return {\"response\": response}\n",
    "\n",
    "def handle_general(state: State) -> State:\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Provide a general support response to the following query: {query}\"\n",
    "    )\n",
    "    response = (prompt | llm).invoke({\"query\": state['query']}).content\n",
    "    return {\"response\": response}\n",
    "\n",
    "def escalate(state: State) -> State:\n",
    "    return {\"response\": \"This query has been escalated to a human agent due to negative sentiment.\"}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Routing logic\n",
    "def route_query(state: State):\n",
    "    if state['sentiment'] == \"Negative\":\n",
    "        return \"escalate\"\n",
    "    elif state['category'] == \"Technical\":\n",
    "        return \"handle_technical\"\n",
    "    elif state['category'] == \"Billing\":\n",
    "        return \"handle_billing\"\n",
    "    else:\n",
    "        return \"handle_general\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build LangGraph workflow\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"categorize\", categorize)\n",
    "workflow.add_node(\"analyze_sentiment\", analyze_sentiment)\n",
    "workflow.add_node(\"handle_technical\", handle_technical)\n",
    "workflow.add_node(\"handle_billing\", handle_billing)\n",
    "workflow.add_node(\"handle_general\", handle_general)\n",
    "workflow.add_node(\"escalate\", escalate)\n",
    "\n",
    "workflow.set_entry_point(\"categorize\")\n",
    "workflow.add_edge(\"categorize\", \"analyze_sentiment\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"analyze_sentiment\",\n",
    "    route_query,\n",
    "    {\n",
    "        \"handle_technical\": \"handle_technical\",\n",
    "        \"handle_billing\": \"handle_billing\",\n",
    "        \"handle_general\": \"handle_general\",\n",
    "        \"escalate\": \"escalate\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"handle_technical\", END)\n",
    "workflow.add_edge(\"handle_billing\", END)\n",
    "workflow.add_edge(\"handle_general\", END)\n",
    "workflow.add_edge(\"escalate\", END)\n",
    "\n",
    "app = workflow.compile()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize graph\n",
    "display(Image(app.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Helper function to run agent\n",
    "def run_customer_support(query: str) -> Dict[str, str]:\n",
    "    result = app.invoke({\"query\": query})\n",
    "    return {\n",
    "        \"Category\": result['category'],\n",
    "        \"Sentiment\": result['sentiment'],\n",
    "        \"Response\": result['response']\n",
    "    }"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Gradio interface\n",
    "import gradio as gr\n",
    "\n",
    "def gradio_interface(query: str):\n",
    "    result = run_customer_support(query)\n",
    "    return (\n",
    "        f\"**Category:** {result['Category']}\\n\\n\"\n",
    "        f\"**Sentiment:** {result['Sentiment']}\\n\\n\"\n",
    "        f\"**Response:** {result['Response']}\"\n",
    "    )\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter customer query...\"),\n",
    "    outputs=gr.Markdown(),\n",
    "    title=\"Customer Support Agent\",\n",
    "    description=\"AI-powered customer support using LangGraph and Groq\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
